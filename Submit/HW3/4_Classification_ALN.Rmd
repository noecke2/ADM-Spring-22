---
title: "Classification-2"
author: "Andrew Noecker"
date: "2/23/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
library(dslabs)
```

# Is it a 2 or 7? - continuation

Last time we started considering the problem of labeling digits as `2` or a `7` using the following variables (features):

* `x_1` will be the proportion of dark pixels in the upper left quadrant.
* `x_2` will be the proportion of dark pixels in the lower right quadrant.

Let's start by loading the dataset `mnist_27` from `dslabs` and creating our testing and training datasets:

```{r echo=TRUE}
data("mnist_27")
mnist.train.tbl <- tibble(mnist_27$train)
mnist.test.tbl <- tibble(mnist_27$test)
```

And let's note the dimensions of those datasets

```{r echo=TRUE}
dim(mnist.train.tbl)
dim(mnist.test.tbl)
```

Today we are interested in defining the *decision boundary* of the best theoretical classifier which we will call the *Bayes' boundary*. The `mnist` dataset has over 60,000 digits so we can approximate the theoretical probability of a 7 (compared to a 2). Luckily for us this information is contained in the field `true_p` of `mnist_27`. Let's take a look at it:

```{r echo=TRUE}
mnist.true.tbl <-  tibble(mnist_27$true_p)
```

The way to interpret this table is to note that given `x_1` and `x_2` it provides an estimate of the probability of a digit been a `7`. Let's plot how this probability looks like in two similar ways

```{r echo=TRUE}
ggplot(mnist.true.tbl, aes(x_1, x_2, fill = p)) +
  geom_raster() +
  scale_fill_viridis_c()
```

```{r echo=TRUE}
ggplot(mnist.true.tbl, aes(x_1, x_2, fill = p)) +
  geom_raster() +
  scale_fill_viridis_b()
```

Notice how points on the far right are likely to be `7`, and how points in the left are very likely to be `2`. Finally notice the probability changes around a curved region in the left of the screen.

The Bayes' boundary consists of all the points where the probability is exactly equal to 0.5. We can plot this boundary by using the `stat_contour` command of `ggplot`. Notice that for `stat_contour` to work you need to define `z` in your `aes` command:

```{r echo=FALSE}
ggplot(mnist.true.tbl, aes(x_1, x_2, z=p,fill = p)) +
  geom_raster() +
  stat_contour(breaks=c(0.5), color="black")+
  scale_fill_viridis_b()
```

In the following exercises we will explore the decision boundary generated by our KNN classifier using the following steps:


1. Using a value of `kNear` of 10, create a KNN model using your training dataset

```{r}
kNear = 10
knn.model <- knn3(y ~ x_1+x_2, data = mnist.train.tbl, k = kNear)
```


2. We would like to visualize the values of our KNN model across all of the points of the unit square. However our testing dataset does not contain enough of those points so we need to create a tibble with a big amount of points from the unit square interval. We will do that in the following steps

a. Create a vector `grid.vec` that contains the numbers 0, 0.1, ...,1. Make use of the function `seq`.

```{r}
grid.vec <- seq(from = 0, to = 1, by = 0.1)
```


b. Look at the documentation of the `expand_grid` function from the `tidyverse`, and create a tibble `grid.tbl` with two columns, `x_1` and `x_2` which contains a grid of combinations of two points from 0 to 1 by steps of 0.1

```{r}
grid.tbl <- expand_grid(x_1 = grid.vec, x_2 = grid.vec)
```



c. Evaluate your KNN model on the values of `grid.tbl`. Create a new column `prob` in `grid.tbl` with the predicted probability of being a 7.

```{r}
prob <- predict(knn.model, newdata = grid.tbl)
grid.tbl <- grid.tbl%>%
  mutate(prob2 = prob[,1],
         prob7 = prob[,2])
```


d. Use `grid.tbl` to plot the predicted probability across the unit grid and plot the *decision boundary*.

```{r}
ggplot(grid.tbl, aes(x_1, x_2, z=prob7,fill = prob7)) +
  geom_raster() +
  stat_contour(breaks=c(0.5), color="black")+
  scale_fill_viridis_b()
```


e. It seems your graph is too pixelated. Create a function `plot_knn_model(kNear, grid.dist, train.tbl)` that trains a KNN model with parameter `kNear` on `train.tbl` and displays the value of the probability of being a 7 on a grid of points generated every `grid.dist`. Evaluate your function using `grid.dist` equals to 0.1, 0.03 and 0.01

```{r}
plot_knn_model <- function(kNear, grid.dist, train.tbl){
  
  knn.model <- knn3(y ~ x_1+x_2, data = train.tbl, k = kNear)
  
  grid.vec <- seq(from = 0, to = 1, by = grid.dist)
  grid.tbl <- expand_grid(x_1 = grid.vec, x_2 = grid.vec)
  
  prob <- predict(knn.model, newdata = grid.tbl)
  grid.tbl <- grid.tbl%>%
  mutate(prob2 = prob[,1],
         prob7 = prob[,2])
  
  ggplot(grid.tbl, aes(x_1, x_2, z=prob7,fill = prob7)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="black")+
    scale_fill_viridis_b()+
    labs(title = str_c("With grid.dist = ", grid.dist, " and kNear = ", kNear))
}


plot_knn_model(10, 0.1, mnist.train.tbl)
plot_knn_model(10, 0.03, mnist.train.tbl)
plot_knn_model(10, 0.01, mnist.train.tbl)

```



3. Experiment plotting with different values of k (say from k=5 to k=50, using steps of 5). Which decision boundary looks more similar to the Bayes boundary? Is this consistent with the optimal value of `k` that you found using in point 4 of our last activity, `3_Classification.Rmd`?

```{r}
plot_knn_model(5, 0.01, mnist.train.tbl)
plot_knn_model(10, 0.01, mnist.train.tbl)
plot_knn_model(15, 0.01, mnist.train.tbl)
plot_knn_model(20, 0.01, mnist.train.tbl)
plot_knn_model(25, 0.01, mnist.train.tbl)
plot_knn_model(30, 0.01, mnist.train.tbl)
plot_knn_model(35, 0.01, mnist.train.tbl)
plot_knn_model(40, 0.01, mnist.train.tbl)
plot_knn_model(45, 0.01, mnist.train.tbl)
plot_knn_model(50, 0.01, mnist.train.tbl)

```

I think that having the higher kNear leads to the more similar Bayes boundary. With kNear = 50 we have a decision boundary that looks pretty similar to what we saw in the preface of this section, but kNear = 40 yields a pretty similar one as well. In the prior document, we found the optimal kNear to be 41, which is pretty close with what we found here, so our results mostly line up. 




# Decision boundary of a linear classifer

4. We can also use a linear model to approximate the probability of being a `7`. Notice that in order to make this approach work, we need to create a new input variable where `2`s are encoded as zeros and `7`s are encoded as ones. Also note that the linear model can give values outside of $[0,1]$ so we will need to truncate predictions that are negative to 0 and prediction over 1 to 1. Implement this approach and plot the boundary of this classifier. How does this boundary compare to the boundary generated by the KNN model?

```{r}
mnist.train.tbl <- mnist.train.tbl%>%
  mutate(is7 = ifelse(y == "7", 1, 0))

mnist.lm <- lm(is7 ~ x_1+x_2, data = mnist.train.tbl)

new.grid.vec <- seq(0, 1, 0.01)
new.grid.tbl <- expand_grid(x_1 = new.grid.vec, x_2 = new.grid.vec)

prob <- predict(mnist.lm, new.grid.tbl)

new.grid.tbl2 <- new.grid.tbl %>%
  mutate(prob7new = prob)%>%
  mutate(prob7new = ifelse(prob7new < 0, 0, prob7new),
         prob7new = ifelse(prob7new > 1, 1, prob7new))

ggplot(new.grid.tbl2, aes(x_1, x_2, z=prob7new, fill = prob7new)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="black")+
    scale_fill_viridis_b()

```

This boundary is (obviously) a straight line, but it's much simpler/dry cut than the boundary we generated with the KNN model. While the KNN model boundary may overfit at times, this one appears to be somewhat simplistic. 



# The Default dataset

In the following sets of exercises we will be exploring the `Default` dataset available from the `ISLR2` package. In particular we will construct linear models that would allow us to predict whether a particular person would go on default. 

```{r}
library(ISLR2)
data(Default)
default.tbl <- tibble(Default)
```


5. 

    a. Generate a plot of balance (x) and income (y) vs default (using color). What trends do you observe?  
    
```{r}
ggplot(default.tbl, aes(balance, income, color = default))+
  geom_point()
```

Overall, we see that most observations do not default, and those that do are more likely to have higher balances (>2000). We also see almost no defaults if the balance is less than 1000. 
    

    b. Divide the original datasets into a training (8000 elements) and a testing dataset (2000 elements) by selecting at random using `slice_sample` and `setdiff` from `tidyverse`. Please keep the `set.seed` command so that your result is reproducible. 

```{r}
set.seed(12345)

default.train.tbl <- default.tbl%>%
  slice_sample(n = 8000)

default.test.tbl <- setdiff(default.tbl, default.train.tbl)
```


6. Create a linear model (similar to point 4) that predicts `default` based on `balance` and `income`. What is the missclassification rate?

```{r}

#Assigning 0 and 1 to default variable
default.train.tbl <- default.train.tbl%>%
  mutate(isDefault = ifelse(default == "Yes", 1, 0))

#Modeling and Predicting
default.lm <- lm(isDefault ~ balance + income, data = default.train.tbl)
summary(default.lm)

prob1 <- predict(default.lm, default.test.tbl)

#Multiply by 1/max(probDef) --> ensures max value is now 1

default.test.tbl <- default.test.tbl%>%
  mutate(probDef = prob1,
         probDef = ifelse(probDef < 0, 0, probDef),
         probDef = ifelse(probDef > 1, 1, probDef),
         probDef_adj = probDef * (1/max(probDef)),
         predDef = ifelse(probDef_adj >=0.5, "Yes", "No"))


#Misclassification Rate

mean(default.test.tbl$predDef != default.test.tbl$default)
```

After adjusting our probabilities so that the maximum probability is calibrated as '1', we have a missclassification rate of 0.062. 


7. Plot the probability of the model created on 6) on a grid where $(x_1,x_2) \in [0,3000]\times[0,80000]$. Make sure your grid **does not have over 10,000 points**. Plot the decision boundary of the model as well. 

```{r}

#Make grid - (stepped by 31 so that the expanded grid was < 10000)
vec1 <-seq(0, 3000, 31)
vec2 <-seq(0, 80000, 800)

def.grid.tbl <- expand_grid(balance = vec1, income = vec2)

prob2 <- predict(default.lm, def.grid.tbl)

#Same as before - calibrating data
def.grid.tbl <- def.grid.tbl%>%
  mutate(probDef = prob2,
         probDef = ifelse(probDef < 0, 0, probDef),
         probDef = ifelse(probDef > 1, 1, probDef),
         probDef_adj = probDef * (1/max(probDef)),
         predDef = ifelse(probDef_adj >=0.5, "Yes", "No"))

ggplot(def.grid.tbl, aes(balance, income, z=probDef_adj, fill = probDef_adj)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="black")+
    scale_fill_viridis_b()
```


8. Does default change depending on whether somebody is a student or not? Illustrate your answer using a plot using facets.

```{r}
ggplot(default.tbl, aes(balance, income, color = default))+
  geom_point()+
  facet_wrap(~student, nrow = 1)
```

When we facet wrap based on whether the person is a student or not, we don't see much of a difference. As one would expect, the students have significantly lower income, but we see that those who default all have higher balances, regardless of student status. 


9. Create a linear model that uses `student`, `balance`, and `income` to predict `default`. What is the missclassification rate of this model? Are the results better than the model created in 6?


```{r}
#New Model
default2.lm <- lm(isDefault ~ balance + income + student, data = default.train.tbl)
summary(default2.lm)

#Predicting values
prob2 <- predict(default2.lm, default.test.tbl)

#Same as before, calibrating values where max value is 1
default2.test.tbl <- default.test.tbl%>%
  mutate(probDef = prob2,
         probDef = ifelse(probDef < 0, 0, probDef),
         probDef = ifelse(probDef > 1, 1, probDef),
         probDef_adj = probDef * (1/max(probDef)),
         predDef = ifelse(probDef_adj >=0.5, "Yes", "No"))

#Misclassification Rate
mean(default2.test.tbl$predDef != default2.test.tbl$default)
```


When we consider student status in our model, we get a missclassification rate of 0.605, a missclassification rate that is 0.0015 lower than what we found without considering student status. 


10. Plot the probability and the decision boundary for the model created in 9.

```{r}
#Making grid as before, this time with third vector of student
vec1 <-seq(0, 3000, 31)
vec2 <-seq(0, 80000, 800)
vec3 <- factor(c("Yes", "No"))

def.grid2.tbl <- expand_grid(balance = vec1, income = vec2, student = vec3)

prob2 <- predict(default2.lm, def.grid2.tbl)

#Same as before, calibrating predictions
def.grid2.tbl <- def.grid2.tbl%>%
  mutate(probDef = prob2,
         probDef = ifelse(probDef < 0, 0, probDef),
         probDef = ifelse(probDef > 1, 1, probDef),
         probDef_adj = probDef * (1/max(probDef)),
         predDef = ifelse(probDef_adj >=0.5, "Yes", "No"))

#Raster plot faceted by student status
ggplot(def.grid2.tbl, aes(balance, income, z=probDef_adj, fill = probDef_adj)) +
    geom_raster() +
    stat_contour(breaks=c(0.5), color="black")+
    scale_fill_viridis_b()+
  facet_wrap(~student, nrow = 2)
```

