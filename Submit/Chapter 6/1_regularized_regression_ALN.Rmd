---
title: "An Introduction to Regularization"
author: "Jaime Davila"
date: "4/4/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
```

# Introduction: the Credit dataset

Today we will be using the `Credit` dataset from the `ISLR2` package. More information about this dataset can be found in
https://rdrr.io/cran/ISLR2/man/Credit.html. In particular we 
are interested in predicting the balance based on the other variables from the dataset

```{r echo=TRUE}
library(ISLR2)
data(Credit)
credit.tbl <- as_tibble(Credit)
credit.tbl
```

And as usual, we will be creating training and testing datasets using `tidymodels()`

```{r echo=TRUE}
library(tidymodels)
tidymodels_prefer()

set.seed(654321)
credit.split <- initial_split(credit.tbl, prop=0.8)
credit.train.tbl <- training(credit.split)
credit.test.tbl <- testing(credit.split)
```

# A first approach: A linear model

Let's start by creating a recipe called `credit.recipe`. In particular we would like:

* To predict `Balance` using all the other variables as explanatory variables (in R this can be written as `Balance ~.`).

* We would like to encode all categorical variables using indicator variables (using `step_dummy()`). Notice all categorical variables are binary, except for `Region` that has 3 different levels.

```{r echo=TRUE}
credit.recipe <- 
  recipe(formula=Balance ~ ., data=credit.train.tbl) %>%
  step_dummy(all_nominal_predictors())
```

Let's also remember that in linear regression we obtain coefficients $\beta_0$, $\beta_1, \dots, \beta_p$ that minimize

$$ RSS := \sum\limits_{i=1}^n \left(
y_i - \beta_0 - \sum \limits_{i=1}^p \beta_j x_{ij}
\right)^2$$


1. Set up the rest of the model and fit it using the training dataset. Create a table with all the coefficients of your linear model. Interpret the coefficients corresponding to:

* `Income`
* `Student_Yes`
* `Region_South`

What is the value of the $R^2$ on the testing dataset?

```{r}
lm.model <- linear_reg() %>%
  set_engine("lm")

lm.wflow <- workflow() %>%
  add_recipe(credit.recipe) %>%
  add_model(lm.model) 

lm.fit <- fit(lm.wflow, credit.train.tbl)

tidy(lm.fit)

augment(lm.fit, credit.test.tbl)%>%
  rsq(truth = Balance, estimate = .pred)
```
We get an $R^2$ value of 0.946 when we test the model on our testing dataset. 

Income is -7.82, meaning that if all other predictors remain fixed, than an increase of \$1000 in income has an associated decrease of \$7.82 in average credit card balance. 

For student_yes, this means that if all other coefficients are held constant, then we'd expect a student to have a balance that is $422 greater than a non student. 

For Region_South, the coefficient of 4.06 means that if all other predictors are held equal, then we'd expect someone in the South to have a balance that is \$4.06 higher than someone in the east. 


2. We can rate the importance of the different variables in a linear model by using the absolute value of their $t$-statistic (fourth term in your tibble generated by `tidy()`). Sort the variables according to the absolute value of the statistic. Compare your results with the graph generated by the `vip()` function available below:

```{r echo=TRUE}
library(vip)

tidy(lm.fit)%>%
  arrange(desc(abs(statistic)))

extract_fit_parsnip(lm.fit) %>%
  vip()
```
It all lines up!


# A first attempt at regularization: Ridge regression

Ridge regression works by changing the objective function that we want to minimize. Instead of just minimizing the RSS, we minimize the following function:


$$RSS + \lambda \sum\limits_{i=1}^p \beta_j^2$$
Notice that the $\beta_i$ are the coefficients of your linear model and $\lambda$ is a parameter to be defined. If $\lambda=0$ the problems becomes the same as linear regression and as $\lambda$ increases it has the effect of reducing the coefficients $\beta_i$ towards zero.

We can set up our ridge regression as follows:

```{r echo=TRUE}
ridge.model <- 
  linear_reg(mixture = 0, penalty=1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")
```

Notice that `mixture=0` selects a ridge regression and `mixture=1` selects a lasso regression (more on this later). Finally the `penalty` is the value that we give to $\lambda$ in our minimization function.

When we are dealing with a regularized model we need to make sure to normalize all of our explanatory variables using `step_normalize` as below:

```{r echo=TRUE}
ridge.recipe <- 
  recipe(formula = Balance ~ ., data = credit.train.tbl) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

And finally we can put it all together using:

```{r echo=TRUE}
ridge.wf <- workflow() %>% 
  add_recipe(ridge.recipe) %>% 
  add_model(ridge.model)

ridge.fit <- fit(ridge.wf, credit.train.tbl)
tidy(ridge.fit)
```

3. 
   a. Use the `vip` function to see the relative importance of each input variable and compare it with the results that you obtain from your linear model
   
```{r}
extract_fit_parsnip(ridge.fit)%>%
  vip()
```
   
   

   b. Notice that you can change the value of the penalty (or $\lambda$) as a parameter in the function `tidy()`. Experiment with several values of penalties (for example $\lambda=32,128,256$) and see what is the effect on the coefficients of your ridge model
   
```{r}
tidy(ridge.fit, penalty = 32128256) #Basically all 0
tidy(ridge.fit, penalty = 100)
tidy(ridge.fit)
tidy(ridge.fit, penalty = 50)
tidy(ridge.fit, penalty = 10000)
```

There is generally a negative relationship between the penalty and the absolute value of the coefficient (if penalty increases, abs(coefficient)) decreases. If we have lambda be REALLY big, then we get coefficients that are all basically 0. 
   

4.  Compare the following plot with the left panel of figure 6.4 from ISLR. How do you interpret the graph? How do you interpret the line in black and to what coefficient does it correspond to?

```{r echo=TRUE}
ridge.fit %>%
  extract_fit_engine() %>%
  plot(xvar = "lambda")
```

The black line represents income. It represents the effect of income on balance as lambda increases. It starts as negative, becomes positive, and then converges to 0. All the lines represent a different predictor, with the y axis representing their coefficient value at the respective log lambda value. 

# Calculating the optimal $\lambda$

In order to calculate the optimal $\lambda$ we will be using cross-validation on our training dataset. The following exercise will guide you in how to accomplish this:


5. Use 10  fold cross-validation to find the optimal $\lambda$ for this model by doing the following steps

a. Create a 10 fold cross validation tibble from `credit.train.tbl`

```{r}
#Hitters_fold <- vfold_cv(Hitters_train, v = 10)
credit.fold <- vfold_cv(credit.train.tbl, v = 10)

```


b. Create a ridge model specifying that the `penalty` will be **tuned**

```{r}
ridge.spec <- 
  linear_reg(penalty = tune(), mixture = 0) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

ridge.workflow <- workflow() %>% 
  add_recipe(ridge.recipe) %>% 
  add_model(ridge.spec)


```


c. Create a penalty grid that takes 20 values between -2 to 2

```{r}
penalty.grid <- grid_regular(penalty(range = c(-2, 2)), levels = 20)
penalty.grid
```


d. Use `tune_grid()` to evaluate the model on the different parameters of the grid and plot the effect of the parameter on the fit of the model. 

```{r}
tune_res <- tune_grid(
  ridge_workflow,
  resamples = credit.fold, 
  grid = penalty.grid
)

autoplot(tune_res)

```


e. Select the best penalty minimizing $R^2$  and calculate the $R^2$ of this model on your testing dataset. Compare the $R^2$ with the one you obtained using your linear model.

```{r}
show_best(tune_res, metric = "rsq")

best_penalty <- select_best(tune_res, metric = "rsq")
best_penalty


ridge.final.wf <- finalize_workflow(ridge.wf, best_penalty)
ridge.final.fit <- fit(ridge.final.wf, credit.train.tbl)

augment(ridge.final.fit, credit.test.tbl)%>%
  rsq(truth = Balance, estimate = .pred)

```

We get an $R^2$ of 0.932, whereas ordinary least squares regression yielded an $R^2$ of 0.946. This shows that the ordinary least squares explained a higher percentage of our variability than the ridge models. 

